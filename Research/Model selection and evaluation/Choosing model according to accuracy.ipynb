{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19y0mIADnGKrR6XiDYIljjYTUk1BPTWZ1","timestamp":1687337829927}],"machine_shape":"hm","collapsed_sections":["LWd1UlMnhT2s","ltuuko9Ay7D-","K1VMqkGvhc3-","c-e95z-8h87o","70cBPboUvD0q","FAzzFHcHkHsi","SZBjG3bYbqj8","aKLFELLZleiU"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0MRC0e0KhQ0S"},"source":["# Choosing model according to accuracy"]},{"cell_type":"markdown","metadata":{"id":"LWd1UlMnhT2s"},"source":["## Import libraries"]},{"cell_type":"code","metadata":{"id":"YvGPUQaHhXfL"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from google.colab import files\n","from skimage.io import imsave, imread\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","import joblib\n","\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.cluster import KMeans\n","from sklearn.tree import DecisionTreeClassifier\n","from xgboost import XGBClassifier\n","\n","# from catboost import CatBoostClassifier #!pip install catboost"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Functions"],"metadata":{"id":"ltuuko9Ay7D-"}},{"cell_type":"code","source":["import numpy as np\n","\n","def Round_Array(arr, num_digits):\n","    rounded_arr = [[round(x, num_digits) for x in row] for row in arr]\n","    return np.array(rounded_arr)"],"metadata":{"id":"4XFrdP_Ny9sw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Replace_1_To_255(arr):\n","    for i in range(len(arr)):\n","        for j in range(len(arr[i])):\n","            if arr[i][j] == 1:\n","                arr[i][j] = 255\n","    return arr"],"metadata":{"id":"jEViDyUJ3fCz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K1VMqkGvhc3-"},"source":["## Importing the dataset"]},{"cell_type":"code","metadata":{"id":"M52QDmyzhh9s"},"source":["training_file_name = 'Duck backward elimination with support vector classification (10 featurs selected) - prepared data' + '.csv'\n","data = pd.read_csv(training_file_name)\n","\n","# The file is read with a column of indexes and this line removes it\n","data = data.drop(data.columns[0], axis=1)\n","\n","# Remove the segmented image layer\n","X = data.drop('segmented', axis=1)\n","\n","y = data['segmented']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Spliting to train and sest by classes"],"metadata":{"id":"c-e95z-8h87o"}},{"cell_type":"code","source":["# Split the data by classes\n","data_class_1 = data.loc[data['segmented'] == 1]\n","data_class_0 = data.loc[data['segmented'] == 0]"],"metadata":{"id":"-Yrv4rPwx_Wv"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVzJWAXIhxoC"},"source":["# Split to train and test\n","test_size = 900\n","train_size = 2000\n","class_1_train, class_1_test = train_test_split(data_class_1, test_size=test_size , train_size=train_size, random_state = 0)\n","class_0_train, class_0_test = train_test_split(data_class_0, test_size=test_size , train_size=train_size, random_state = 0)\n","\n","# Concat the train set of the two classes and shuffle it\n","train = pd.concat([class_1_train, class_0_train], axis=0)\n","train = train.sample(frac=1, random_state=0)\n","\n","# Concat the test\n","test = pd.concat([class_1_test, class_0_test], axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = train.drop('segmented', axis=1)\n","y_train = train['segmented']\n","\n","X_test = test.drop('segmented', axis=1)\n","y_test = test['segmented']"],"metadata":{"id":"aDESsG5yqhx5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train the model"],"metadata":{"id":"70cBPboUvD0q"}},{"cell_type":"code","source":["model_name = 'XGBClassifier'\n","classifier = XGBClassifier().fit(X_train, y_train)"],"metadata":{"id":"V0J6trOtGwVd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Accuracy assessment"],"metadata":{"id":"FAzzFHcHkHsi"}},{"cell_type":"code","source":["# Import new image for test\n","training_file_name = 'Geometry (10 featurs selected) - prepared data (only for testing)' + '.csv'\n","test_data = pd.read_csv(training_file_name)\n","\n","# The file is read with a column of indexes and this line removes it\n","test_data = test_data.drop(test_data.columns[0], axis=1)\n","\n","# Remove the segmented image layer\n","X_new_image = test_data.drop('segmented', axis=1)\n","\n","y_new_image = test_data['segmented']\n","\n","# Feature scaling\n","X_new_image = StandardScaler().fit_transform(X_new_image)"],"metadata":{"id":"LoRBMDHekWnl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict for new image\n","predicte_new = classifier.predict(X_new_image)\n","\n","# Predict for the train set\n","y_pred_train = classifier.predict(X_train)\n","\n","# Predict for the train set\n","y_pred_test = classifier.predict(X_test)"],"metadata":{"id":"b5Gb9wZrkV3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print confusion matrix and accuracy\n","\n","num_of_digets = 2\n","\n","# New image\n","cm = confusion_matrix(y_new_image, predicte_new)\n","row_sums = cm.sum(axis=1)\n","cm_new_image = (cm / row_sums) * 100\n","cm_new_image = Round_Array(cm_new_image, num_of_digets)\n","\n","accuracy_new_image = accuracy_score(y_new_image, predicte_new) * 100\n","accuracy_new_image = round(accuracy_new_image, num_of_digets)\n","\n","print('New Image:')\n","print(cm_new_image)\n","print('Accuracy = ' + str(accuracy_new_image))\n","print('--------------------------')\n","print('\\n')\n","\n","\n","# Test set\n","cm = confusion_matrix(y_test, y_pred_test)\n","row_sums = cm.sum(axis=1)\n","cm_test = (cm / row_sums) * 100\n","cm_test = Round_Array(cm_test, num_of_digets)\n","\n","accuracy_test = accuracy_score(y_test, y_pred_test) * 100\n","accuracy_test = round(accuracy_test, num_of_digets)\n","\n","print('Test set:')\n","print(cm_test)\n","print('Accuracy = ' + str(accuracy_test))\n","print('--------------------------')\n","print('\\n')\n","\n","\n","# Train set\n","cm = confusion_matrix(y_train, y_pred_train)\n","row_sums = cm.sum(axis=1)\n","cm_train = (cm / row_sums) * 100\n","cm_train = Round_Array(cm_train, num_of_digets)\n","\n","accuracy_train = accuracy_score(y_train, y_pred_train) * 100\n","accuracy_train = round(accuracy_train, num_of_digets)\n","\n","print('Train set:')\n","print(cm_train)\n","print('Accuracy = ' + str(accuracy_train))\n","print('--------------------------')\n","print('\\n')"],"metadata":{"id":"FLZG4o89mlV7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the accuracy of the model\n","\n","accuracy_table = {'new image': [cm_new_image, accuracy_new_image],\n","                  'test set':  [cm_test, accuracy_test],\n","                  'train set': [cm_train, accuracy_train]}\n","\n","accuracy_table = pd.DataFrame(accuracy_table, index=['confusion matrix', 'total accuracy'])"],"metadata":{"id":"3xJmvEdqvMhZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_name = 'Accuracy ' + model_name + '.csv'\n","accuracy_table.to_csv(file_name)\n","files.download(file_name)"],"metadata":{"id":"S0pHuPTC0kwO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Show the images classification"],"metadata":{"id":"SZBjG3bYbqj8"}},{"cell_type":"code","source":["# Train image\n","\n","original_columns = 111\n","original_rows = 108\n","new_columns = original_columns - 6\n","new_rows = original_rows - 6\n","\n","# Show the image\n","image = classifier.predict(X).reshape(new_rows, new_columns)\n","plt.imshow(image)\n","\n","# Save as tiff\n","image_name = 'train image ' + model_name + '.tif'\n","imsave(image_name, np.uint8(Replace_1_To_255(image)))\n","files.download(image_name)"],"metadata":{"id":"0vyQb-BqrSLp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# New image\n","\n","original_columns = 125\n","original_rows = 100\n","new_columns = original_columns - 6\n","new_rows = original_rows - 6\n","\n","# Show the image\n","image = classifier.predict(X_new_image).reshape(new_rows, new_columns)\n","plt.imshow(image)\n","\n","# Save as tiff\n","image_name = 'new image ' + model_name + '.tif'\n","imsave(image_name, np.uint8(Replace_1_To_255(image)))\n","files.download(image_name)"],"metadata":{"id":"k5Ij4DnNp8eZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save the model"],"metadata":{"id":"aKLFELLZleiU"}},{"cell_type":"code","source":["# Save the trained model\n","model_name_to_save = model_name + '.pkl'\n","joblib.dump(classifier, model_name_to_save)\n","files.download(model_name_to_save)"],"metadata":{"id":"dQ7Lv2N4ljYV"},"execution_count":null,"outputs":[]}]}